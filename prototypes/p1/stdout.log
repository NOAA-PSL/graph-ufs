(graphufs) [slurm12 p1]$ python train.py --num-gpus=4 --batch-size=8 --local-store-path=/lustre/p1-data-4gpus/ --latent-size=256
/contrib2/Tim.Smith/graph-ufs/graphufs/training.py:59: UserWarning: Import failed for either mpi4py or mpi4jax.
  warnings.warn("Import failed for either mpi4py or mpi4jax.")
[26 s] [Rank 0] [INFO] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
[26 s] [Rank 0] [INFO] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[26 s] [Rank 0] [INFO] Using 4 GPUs.
[27 s] [Rank 0] [INFO]
jax:    0.4.27
jaxlib: 0.4.23.dev20240503
numpy:  1.26.4
python: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]
jax.devices (4 total, 4 local): [cuda(id=0) cuda(id=1) cuda(id=2) cuda(id=3)]
process_count: 1
platform: uname_result(system='Linux', node='timsmith-p1-00015-3-0001', release='4.18.0-513.18.1.el8_9.x86_64', version='#1 SMP Wed Feb 21 21:34:36 UTC 2024', machine='x86_64')


$ nvidia-smi
Thu May 16 19:17:29 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000001:00:00.0 Off |                    0 |
| N/A   40C    P0             74W /  300W |     425MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000002:00:00.0 Off |                    0 |
| N/A   39C    P0             72W /  300W |     425MiB /  81920MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000003:00:00.0 Off |                    0 |
| N/A   43C    P0             75W /  300W |     425MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000004:00:00.0 Off |                    0 |
| N/A   43C    P0             76W /  300W |     425MiB /  81920MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     18951      C   python                                        416MiB |
|    1   N/A  N/A     18951      C   python                                        416MiB |
|    2   N/A  N/A     18951      C   python                                        416MiB |
|    3   N/A  N/A     18951      C   python                                        416MiB |
+-----------------------------------------------------------------------------------------+

[27 s] [Rank 0] [INFO] Local devices: 4 [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)]
[27 s] [Rank 0] [INFO] Global devices: 4 [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)]
[28 s] [Rank 0] [INFO] Chunks for training: 48
[28 s] [Rank 0] [INFO] Chunk 0: 1993-12-31 18:00:00 to 1994-01-08 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 1: 1994-01-08 06:00:00 to 1994-01-15 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 2: 1994-01-15 18:00:00 to 1994-01-23 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 3: 1994-01-23 06:00:00 to 1994-01-30 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 4: 1994-01-30 18:00:00 to 1994-02-07 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 5: 1994-02-07 06:00:00 to 1994-02-14 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 6: 1994-02-14 18:00:00 to 1994-02-22 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 7: 1994-02-22 06:00:00 to 1994-03-01 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 8: 1994-03-01 18:00:00 to 1994-03-09 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 9: 1994-03-09 06:00:00 to 1994-03-16 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 10: 1994-03-16 18:00:00 to 1994-03-24 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 11: 1994-03-24 06:00:00 to 1994-03-31 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 12: 1994-03-31 18:00:00 to 1994-04-08 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 13: 1994-04-08 06:00:00 to 1994-04-15 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 14: 1994-04-15 18:00:00 to 1994-04-23 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 15: 1994-04-23 06:00:00 to 1994-04-30 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 16: 1994-04-30 18:00:00 to 1994-05-08 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 17: 1994-05-08 06:00:00 to 1994-05-15 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 18: 1994-05-15 18:00:00 to 1994-05-23 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 19: 1994-05-23 06:00:00 to 1994-05-30 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 20: 1994-05-30 18:00:00 to 1994-06-07 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 21: 1994-06-07 06:00:00 to 1994-06-14 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 22: 1994-06-14 18:00:00 to 1994-06-22 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 23: 1994-06-22 06:00:00 to 1994-06-29 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 24: 1994-06-29 18:00:00 to 1994-07-07 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 25: 1994-07-07 06:00:00 to 1994-07-14 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 26: 1994-07-14 18:00:00 to 1994-07-22 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 27: 1994-07-22 06:00:00 to 1994-07-29 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 28: 1994-07-29 18:00:00 to 1994-08-06 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 29: 1994-08-06 06:00:00 to 1994-08-13 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 30: 1994-08-13 18:00:00 to 1994-08-21 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 31: 1994-08-21 06:00:00 to 1994-08-28 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 32: 1994-08-28 18:00:00 to 1994-09-05 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 33: 1994-09-05 06:00:00 to 1994-09-12 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 34: 1994-09-12 18:00:00 to 1994-09-20 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 35: 1994-09-20 06:00:00 to 1994-09-27 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 36: 1994-09-27 18:00:00 to 1994-10-05 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 37: 1994-10-05 06:00:00 to 1994-10-12 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 38: 1994-10-12 18:00:00 to 1994-10-20 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 39: 1994-10-20 06:00:00 to 1994-10-27 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 40: 1994-10-27 18:00:00 to 1994-11-04 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 41: 1994-11-04 06:00:00 to 1994-11-11 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 42: 1994-11-11 18:00:00 to 1994-11-19 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 43: 1994-11-19 06:00:00 to 1994-11-26 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 44: 1994-11-26 18:00:00 to 1994-12-04 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 45: 1994-12-04 06:00:00 to 1994-12-11 15:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 46: 1994-12-11 18:00:00 to 1994-12-19 03:00:00 : 60 time stamps
[28 s] [Rank 0] [INFO] Chunk 47: 1994-12-19 06:00:00 to 1994-12-31 21:00:00 : 102 time stamps
[61 s] [Rank 0] [INFO] Chunks for validation: 3
[61 s] [Rank 0] [INFO] Chunk 0: 2022-01-01 00:00:00 to 2022-01-11 06:00:00 : 83 time stamps
[61 s] [Rank 0] [INFO] Chunk 1: 2022-01-11 09:00:00 to 2022-01-21 15:00:00 : 83 time stamps
[61 s] [Rank 0] [INFO] Chunk 2: 2022-01-21 18:00:00 to 2022-02-01 00:00:00 : 83 time stamps
[96 s] [Rank 0] [INFO] Initializing weights: 0
/contrib2/Tim.Smith/graph-ufs/graphcast/graphcast/autoregressive.py:202: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.
  scan_length = targets_template.dims['time']
/contrib2/Tim.Smith/graph-ufs/graphcast/graphcast/autoregressive.py:115: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.
  num_inputs = inputs.dims['time']
[111 s] [Rank 0] [INFO] Starting Training with:
[111 s] [Rank 0] [INFO]          100 linearly increasing LR steps
[111 s] [Rank 0] [INFO]          668 cosine decay LR steps
[111 s] [Rank 0] [INFO] Training on epoch 1 and chunk 0
[132 s] [Rank 0] [INFO] Started jitting optim_step
/contrib2/Tim.Smith/miniconda3/envs/graphufs/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1836: UserWarning: The jitted function optim_step includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.
  warnings.warn(
2024-05-16 19:19:57.523299: E external/xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  %pad.99 = bf16[81936,192]{1,0} pad(bf16[81936,190]{1,0} %constant.2517, bf16[] %constant.3150), padding=0_0x0_2, metadata={op_name="jit(optim_step)/jit(main)/pmap(fn_passed_to_pmap)/jvp(grid2mesh_gnn)/_embed/grid2mesh_gnn/sequential_2/encoder_nodes_mesh_nodes_mlp/linear_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=bfloat16]" source_file="/contrib2/Tim.Smith/graph-ufs/graphcast/graphcast/typed_graph_net.py" source_line=307}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2024-05-16 19:19:57.535857: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.01266066s
Constant folding an instruction is taking > 1s:

  %pad.99 = bf16[81936,192]{1,0} pad(bf16[81936,190]{1,0} %constant.2517, bf16[] %constant.3150), padding=0_0x0_2, metadata={op_name="jit(optim_step)/jit(main)/pmap(fn_passed_to_pmap)/jvp(grid2mesh_gnn)/_embed/grid2mesh_gnn/sequential_2/encoder_nodes_mesh_nodes_mlp/linear_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=bfloat16]" source_file="/contrib2/Tim.Smith/graph-ufs/graphcast/graphcast/typed_graph_net.py" source_line=307}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
/contrib2/Tim.Smith/miniconda3/envs/graphufs/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1836: UserWarning: The jitted function vloss includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.
  warnings.warn(
/contrib2/Tim.Smith/miniconda3/envs/graphufs/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1836: UserWarning: The jitted function optim_step includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.
  warnings.warn(
[397 s] [Rank 0] [INFO] Finished jitting optim_step
loss = 19.79630, val_loss = 19.30731, mean(|grad|) = 0.01757999: 100%|████████████████████████████████████████| 8/8 [00:04<00:00,  1.88it/s]
Done with chunk 0: 292.5070 seconds

[403 s] [Rank 0] [INFO] Training on epoch 1 and chunk 1
loss = 18.42575, val_loss = 17.77344, mean(|grad|) = 0.01192914: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.43it/s]
Done with chunk 1: 34.6098 seconds

[438 s] [Rank 0] [INFO] Training on epoch 1 and chunk 2
loss = 17.15585, val_loss = 16.89911, mean(|grad|) = 0.00824941: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.44it/s]
Done with chunk 2: 75.6671 seconds

[513 s] [Rank 0] [INFO] Training on epoch 1 and chunk 3
loss = 15.61862, val_loss = 15.49768, mean(|grad|) = 0.00484817: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.44it/s]
Done with chunk 3: 68.8449 seconds

[582 s] [Rank 0] [INFO] Training on epoch 1 and chunk 4
loss = 14.92065, val_loss = 15.03055, mean(|grad|) = 0.00375797: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.42it/s]
Done with chunk 4: 69.2418 seconds

[652 s] [Rank 0] [INFO] Training on epoch 1 and chunk 5
loss = 13.22833, val_loss = 14.12326, mean(|grad|) = 0.00374667: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.42it/s]
Done with chunk 5: 74.2605 seconds

[726 s] [Rank 0] [INFO] Training on epoch 1 and chunk 6
loss = 13.11453, val_loss = 13.58090, mean(|grad|) = 0.00319701: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]
Done with chunk 6: 70.7750 seconds

[797 s] [Rank 0] [INFO] Training on epoch 1 and chunk 7
loss = 13.40759, val_loss = 12.77872, mean(|grad|) = 0.00296840: 100%|████████████████████████████████████████| 8/8 [00:07<00:00,  1.09it/s]
Done with chunk 7: 69.1893 seconds

[866 s] [Rank 0] [INFO] Training on epoch 1 and chunk 8
loss = 12.36227, val_loss = 12.74222, mean(|grad|) = 0.00217970: 100%|████████████████████████████████████████| 8/8 [00:07<00:00,  1.12it/s]
Done with chunk 8: 68.1750 seconds

[934 s] [Rank 0] [INFO] Training on epoch 1 and chunk 9
loss = 11.35214, val_loss = 12.55420, mean(|grad|) = 0.00174796: 100%|████████████████████████████████████████| 8/8 [00:09<00:00,  1.19s/it]
Done with chunk 9: 72.5890 seconds

[1007 s] [Rank 0] [INFO] Training on epoch 1 and chunk 10
loss = 12.41418, val_loss = 11.88449, mean(|grad|) = 0.00444294: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]
Done with chunk 10: 69.4217 seconds

[1076 s] [Rank 0] [INFO] Training on epoch 1 and chunk 11
loss = 10.06082, val_loss = 11.21127, mean(|grad|) = 0.00189549: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.42it/s]
Done with chunk 11: 69.5582 seconds

[1146 s] [Rank 0] [INFO] Training on epoch 1 and chunk 12
loss = 10.76462, val_loss = 11.54565, mean(|grad|) = 0.00177869: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.42it/s]
Done with chunk 12: 70.0051 seconds

[1216 s] [Rank 0] [INFO] Training on epoch 1 and chunk 13
loss = 10.33307, val_loss = 11.17267, mean(|grad|) = 0.00181060: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]
Done with chunk 13: 69.0991 seconds

[1285 s] [Rank 0] [INFO] Training on epoch 1 and chunk 14
loss = 10.00656, val_loss = 10.61655, mean(|grad|) = 0.00172034: 100%|████████████████████████████████████████| 8/8 [00:05<00:00,  1.44it/s]
Done with chunk 14: 67.7490 seconds

